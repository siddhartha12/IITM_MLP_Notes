{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5ec66b-4c66-4414-abd4-468c2cc51891",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.1 - Introduction to SciKit-Learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45f3d0fc-bc70-4af5-9975-ad16c21b7e82",
   "metadata": {},
   "source": [
    "sklearn APIs are organized on the lines of our ML framework\n",
    "* Training data and preprocessing\n",
    "* Model subsumes loss function and optimization procedure\n",
    "* Model selection and evaluation\n",
    "\n",
    "sklearn APIs are well designed with the following principles\n",
    "* consistency: All APIs share a consistent and simple interface\n",
    "* Inspection: All learnable parameters as well as hyperparameters of all estimators are acessibble direcly via public instance variabnles\n",
    "* Nonproliferation of classes: datasets are represented as numpy arrayts or scipy sparse matrix instead of custom designed classes\n",
    "* Composition: existing building blocks are reduced as much as possible\n",
    "* Sensible defaults: values are used for parameters that enables quick baseline building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2513a034-e612-4a17-a632-d6cc68e5ac6b",
   "metadata": {},
   "source": [
    "## types of sklearn objects\n",
    "1. Transformers\n",
    "    * transforms datasets\n",
    "    * transform()\n",
    "    * fit() learns parameters\n",
    "    * fit_transform() firs parameters and transforms dataset\n",
    "2. Estimators\n",
    "    * estimates model parameters based on training data and hyper parameters\n",
    "    * fit() method\n",
    "3. Predictors\n",
    "    * Makes prediction on dataset\n",
    "    * predict() method that takes datasets as an input and returns predictions\n",
    "    * score() method to measure quality of predictions\n",
    "\n",
    "## Data API\n",
    "Loading generating and preprocessing\n",
    "* sklearn.datasets = loading datasets - custom as well as popular reference datset\n",
    "* sklearn.preprocessing = scaling, centering, normalization and binarization methods\n",
    "* sklean.impute = filling missing values\n",
    "* sklearn.feature_selection = implements feature selection algorithms\n",
    "* sklearn.feature_extraction = Implements feature extraction from raw data\n",
    "\n",
    "## Model API\n",
    "Implement supervised and unsupervised models\n",
    "### Regression\n",
    "* sklearn.linear_model - linear, ridge, lasso models\n",
    "* sklearn.trees\n",
    "\n",
    "### Classification\n",
    "* sklearn.linear model\n",
    "* sklearn.svm\n",
    "* sklearn.trees\n",
    "* sklearn.neighbours\n",
    "* sklearn.naive_bayes\n",
    "* sklearn.multiclass\n",
    "\n",
    "### Multioutput\n",
    "Implements multioutput calssification and regression\n",
    "* sklearn.multioutput\n",
    "\n",
    "### Clustering\n",
    "implements clustering algorithms\n",
    "* sklearn.cluster\n",
    "\n",
    "## Model evaluation API\n",
    "sklearn.metrics implements different metric for model evaluation\n",
    "\n",
    "## Model selection API\n",
    "sklearn.model_selection implements various selection strategies like cross-validation, tuning-hyper-parameters and plotting learning curves\n",
    "\n",
    "## Model inspection API\n",
    "sklearn.model_inspection\n",
    "\n",
    "use documentation using ?function_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc38731d-20ba-4d05-85c1-e9977c3db483",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.2 - Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e35f66-5d2c-449c-9727-a43e60dcbec0",
   "metadata": {},
   "source": [
    "General API has three main kind of interfaces\n",
    "* loaders - used to load toy datasets bundled with sklearn - learn_*\n",
    "* fetchers - used to download and load datasets from the internet - fetch_*\n",
    "* generators - used to generate controlled synthetic datasets - make_*\n",
    "\n",
    "both loaders and fetchers return a `Bunch` object which is a dictionary with two keys of our interest\n",
    "Key - (Data, target)\n",
    "Values - Array \n",
    "\n",
    "generators return tuple (X, y)\n",
    "\n",
    "## Dataset loaders\n",
    "bundled with sklearn and do not require to download them from external sources\n",
    "* load_iris - classification\n",
    "* load_diabeter - regression\n",
    "* load_digits - classification\n",
    "* load_linnerud - multioutput\n",
    "* load_wine - classification\n",
    "* load_breast_cancer - classification\n",
    "\n",
    "### loading external datasets\n",
    "* fetch_openml() - datasets from openml.org\n",
    "* pandas.io - tools to read common formats\n",
    "* scipy.io - specializes in scientific compyuting like .mat or .arff\n",
    "* numpy/routines.io - leoading columnar data\n",
    "* dataset.load_files - directories of text files\n",
    "* dataset.load_svmlight_files() - loads data in svmlight and libsvm sparse format\n",
    "* skimage.io - provides tools to load images and videos in numpy arrays\n",
    "* scipy.io.wavfile.read - reading wav file\n",
    "\n",
    "## Dataset fetchers\n",
    "download and load datasets from outside\n",
    "\n",
    "## generators\n",
    "1. Regression:\n",
    "    * make_regression() - makes regression targets as a sparce random linear combination of features with noise\n",
    "2. Classification:\n",
    "    * make_blobs() and make_classification() first creates a bunch of normally distributed clusters of points and then assign one or more clusters to each class thereby creating multiclass datasets\n",
    "    * make_multi_classification() generates random sampled with multiple labels with a specific generative process and rejection sampling\n",
    "3. Clustering\n",
    "    * make_blobs\n",
    "\n",
    "For managing numerical data, sklearn recommends using optimized file format like HDF5 to reduce data load times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33949792-3630-495e-800d-f6d19fcd77fb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.3 - demonstration of sklearn dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9c268-d7f2-4859-9aed-df1a8566b404",
   "metadata": {},
   "source": [
    "## Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63ee57d-fa05-4260-b5eb-c9cb608c69ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c66ae6-ac44-4827-85c9-caad60531fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils._bunch.Bunch"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading iris datset\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da4ef9ff-8418-4683-9205-00cb565e9b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal length (cm)',\n",
       " 'sepal width (cm)',\n",
       " 'petal length (cm)',\n",
       " 'petal width (cm)']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ccfa9b-2a01-482d-9d5c-801e2183b859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['setosa', 'versicolor', 'virginica'], dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea1b17a-b591-4d41-b4ad-4dd1abf7b790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0144a6f6-dfe9-4f55-8a8d-05483ba8a0ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15823b4a-1418-434c-bc22-0b251e6b12bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_X_y\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_frame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Load and return the iris dataset (classification).\n",
       "\n",
       "The iris dataset is a classic and very easy multi-class classification\n",
       "dataset.\n",
       "\n",
       "=================   ==============\n",
       "Classes                          3\n",
       "Samples per class               50\n",
       "Samples total                  150\n",
       "Dimensionality                   4\n",
       "Features            real, positive\n",
       "=================   ==============\n",
       "\n",
       "Read more in the :ref:`User Guide <iris_dataset>`.\n",
       "\n",
       ".. versionchanged:: 0.20\n",
       "    Fixed two wrong data points according to Fisher's paper.\n",
       "    The new version is the same as in R, but not as in the UCI\n",
       "    Machine Learning Repository.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "return_X_y : bool, default=False\n",
       "    If True, returns ``(data, target)`` instead of a Bunch object. See\n",
       "    below for more information about the `data` and `target` object.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "as_frame : bool, default=False\n",
       "    If True, the data is a pandas DataFrame including columns with\n",
       "    appropriate dtypes (numeric). The target is\n",
       "    a pandas DataFrame or Series depending on the number of target columns.\n",
       "    If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
       "    DataFrames or Series as described below.\n",
       "\n",
       "    .. versionadded:: 0.23\n",
       "\n",
       "Returns\n",
       "-------\n",
       "data : :class:`~sklearn.utils.Bunch`\n",
       "    Dictionary-like object, with the following attributes.\n",
       "\n",
       "    data : {ndarray, dataframe} of shape (150, 4)\n",
       "        The data matrix. If `as_frame=True`, `data` will be a pandas\n",
       "        DataFrame.\n",
       "    target: {ndarray, Series} of shape (150,)\n",
       "        The classification target. If `as_frame=True`, `target` will be\n",
       "        a pandas Series.\n",
       "    feature_names: list\n",
       "        The names of the dataset columns.\n",
       "    target_names: ndarray of shape (3, )\n",
       "        The names of target classes.\n",
       "    frame: DataFrame of shape (150, 5)\n",
       "        Only present when `as_frame=True`. DataFrame with `data` and\n",
       "        `target`.\n",
       "\n",
       "        .. versionadded:: 0.23\n",
       "    DESCR: str\n",
       "        The full description of the dataset.\n",
       "    filename: str\n",
       "        The path to the location of the data.\n",
       "\n",
       "        .. versionadded:: 0.20\n",
       "\n",
       "(data, target) : tuple if ``return_X_y`` is True\n",
       "    A tuple of two ndarray. The first containing a 2D array of shape\n",
       "    (n_samples, n_features) with each row representing one sample and\n",
       "    each column representing the features. The second ndarray of shape\n",
       "    (n_samples,) containing the target samples.\n",
       "\n",
       "    .. versionadded:: 0.18\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Let's say you are interested in the samples 10, 25, and 50, and want to\n",
       "know their class name.\n",
       "\n",
       ">>> from sklearn.datasets import load_iris\n",
       ">>> data = load_iris()\n",
       ">>> data.target[[10, 25, 50]]\n",
       "array([0, 0, 1])\n",
       ">>> list(data.target_names)\n",
       "[np.str_('setosa'), np.str_('versicolor'), np.str_('virginica')]\n",
       "\n",
       "See :ref:`sphx_glr_auto_examples_decomposition_plot_pca_iris.py` for a more\n",
       "detailed example of how to work with the iris dataset.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\siddh\\appdata\\local\\programs\\python\\python313\\lib\\site-packages\\sklearn\\datasets\\_base.py\n",
       "\u001b[1;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda9a3e4-a895-4db2-b4b4-bedfe8c79119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix, label_vector = load_iris(return_X_y=True)\n",
    "feature_matrix.shape, label_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c69c4-270c-4614-917f-02673fbc0a79",
   "metadata": {},
   "source": [
    "## fetchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0d99822-de4f-4cfb-9990-b289d58ed27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing_data = fetch_california_housing()\\\n",
    "\n",
    "housing_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5975eca-0850-44ef-8456-10743ef9af50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch openml\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5549deae-24f6-4bc7-ac03-bb530e599d84",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc8917dc-7127-4545-ac88-a0f9f1175d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 5), (100,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, n_targets=1, shuffle=True, random_state=42)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d47f63-29dd-4483-9c1b-1478f71e448b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 5), (100, 5))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multi regression\n",
    "X, y = make_regression(n_samples=100, n_features=5, n_targets=5, shuffle=True, random_state=42)\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53acc4b0-00a9-4409-870a-25026cf7ef16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10), (100,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classification\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples = 100, n_features=10, n_classes=2, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c032a23e-1b90-4045-be41-2f264702f04b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 10), (100, 5))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multilabel\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "\n",
    "X, y = make_multilabel_classification(n_samples=100, n_features=10, n_classes=5, n_labels=2)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78a536-192e-46a0-acd2-eeccd3b6c4b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.4 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6991429b-969d-49e7-a131-9f03f55c16e6",
   "metadata": {},
   "source": [
    "real world training data is not clean and have issues such as missing values, features on different scales, non-numeric attributes etc\n",
    "\n",
    "often there is a need to pre process data to make it amenable for training model, sklearn provides a rich set of transformaers for this job\n",
    "\n",
    "same pre-processing should be applied to both training and testing dataset\n",
    "sklearn probides pipeline for making it easier to chain multiple ransforms together and apply them uniformly accross train, eval and test\n",
    "\n",
    "Once you get data, first job to explore data and list down preprocessing needed \n",
    "\n",
    "## Pre processing methods\n",
    "* Data Cleaning - sklearn.preprocessing\n",
    "* Feature extraction - sklearn.feature_extraction\n",
    "* Feature Reduction - sklearn.decomposition.pca\n",
    "* feature expansion - sklearn.kernel_approximatoin\n",
    "\n",
    "## transformer methods\n",
    "* fit() - method learn model parameters from a training set\n",
    "* transform() - applies the learn transformation to new data\n",
    "* fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a95d37f-1976-459a-bf9f-83da143bfcfe",
   "metadata": {},
   "source": [
    "## feature extraction\n",
    "sklearn.feature_extraction has useful APIs to extract features from data\n",
    "* DictVectorizer - converts list of mapping of feature name and feature value into a matrix\n",
    "* Feature Hasher\n",
    "    * High-speed, low memoty vectorizer that uses feature hashing technique\n",
    "    * instead of building a hash table of features, as the vectorizers do, it applies a hash function to the mfeatures to determine their column index in sample matrices directly\n",
    "    * this results in increased speed and reduced memory usage at the expense of inspectability, hasher does not remember what the input features looked like and has no inverse_transform method\n",
    "\n",
    "* sklearn.feature_extraction.image.* = for image dataset\n",
    "* sklearn.feature_extraction.text.* = had functions for text datasets\n",
    "\n",
    "## dealing with missing values\n",
    "sklearn.impute - provides functionality to fill missing values in a dataset, MissingIndicator provides indicators for missing values\n",
    "* SimpleImpute - fills with either mean, median, most_frequent and constant\n",
    "* KNNInputer - uses K nearest neighbours approach to fill missing values in a dataset, mean values of n closest neighbours based on euclidean distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade8c90-6f4d-40b6-be0f-3579e251d0a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.4.2 - handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e86ef-f49f-453e-9203-905bc84ea118",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna()\n",
    "# returns the matrix of missing or available\n",
    "\n",
    "df.isna().sum()\n",
    "\n",
    "# this will show unique values of a columns\n",
    "df['age'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce15941-7bee-4564-b82c-466072657570",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.5 - Categorical transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9803a521-87a4-4570-b909-d07f5c50f700",
   "metadata": {},
   "source": [
    "1. OneHotEncoder\n",
    "    * encodes categorical feature or label as one hot numeric array\n",
    "2. LabelEncoder\n",
    "    * encodes **target** variables with value between 0 and k-1 (k-> number of distinct values)\n",
    "3. Ordinal Encoder\n",
    "    * encodes categorical variables with values between 0 and k-1\n",
    "4. LabelBinarizer\n",
    "    * several regression and binary classification can be extended to multi class setup in one v all fasion\n",
    "    * involves training a single regressor or classifier per class\n",
    "5. MultiLabelBinarizer|\n",
    "    * Encodes categorical features with values between 0 and k-1\n",
    "6. add_dummy_feature\n",
    "    * add column with all values 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3eecb-e97d-4ebc-ab58-bfdec3aa2d66",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.6 - Numeric transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4def67a5-0b26-49ac-b369-e70fdc4d6df3",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "* numerical features with different scales leads to slower convergence of iterative optimization procedures\n",
    "* good practice to scale numerical features so that they are on the same scale\n",
    "\n",
    "1. StandardScaler\n",
    "    * transforms the original feature vector x into new of deviations from mean\n",
    "2. MinMaxScaler\n",
    "    * x* = (x-x.min)/(x.max - x.min)\n",
    "3. MaxAbsScaler\n",
    "    * transforms vector x so that all falls into new feature of range [-1,1]\n",
    "    * Maxabsvalue = max(x.max, |x.min|)\n",
    "4. FunctionTransformer\n",
    "    * applying user defined function\n",
    "5. Polynomial transform\n",
    "    * fenerates a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree\n",
    "6. KBinsDiscretizer\n",
    "    * divides a continuous variable into bins then one hot coding further applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b42b56e-95dd-4aa1-9a61-a59859c31da5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.7&8 - Filter and Wrapper Based Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187a7cc5-b6ca-43b2-803d-cc2094777dea",
   "metadata": {},
   "source": [
    "all features do not contribute towards fitting a model, features that do not contribute can be removed, decrease in size of dataset and computation cost of fitting a model. sklearn.feature_selection provides APIs to accomplish this task\n",
    "## Filter\n",
    "1. VarianceThreshold\n",
    "    * Removes features with variance below a certain threshold\n",
    "2. SelectKBest - removes all but the k highest scoring features\n",
    "4. SelectPercentile - removes all but a user specified highest scoring percentage of features\n",
    "6. GenericUnivariateSelect - Performs univariate feature selection with a configurable strategy which can be found by hyper parameter searcj\n",
    "    * SelectFpr selects features based on a false positive rate test\n",
    "    * SelectFds selects features based on an estimated false discovery rate\n",
    "\n",
    "### Univariate scoring function\n",
    "* Each API need a scoring function to score each feature\n",
    "* Three classes of scoring functions are there: mutual information, Chi sq, and F-statistics\n",
    "* MI and F can ve used in both classification and regression problems\n",
    "    * mutual_info_regression, mutual_info_classif\n",
    "    * f_regression, f_classif\n",
    "* Chi sq only for classifications - chi2\n",
    "\n",
    "### Mutual Information\n",
    "* Measures dependency between two variables, returns a non negative value, higher the value, higher dependency\n",
    "\n",
    "### Chi2\n",
    "* easured dependence between two variables\n",
    "* Computes chi sq stats between two non negative feature (boolean or freq) and class label\n",
    "* high indicates higher degree of correlation\n",
    "## Wrapper\n",
    "1. Recursive Feature Elimination (RFE)\n",
    "    * uses an estimator to recursively remove features\n",
    "        * initially fits an estimator on all features\n",
    "    * obtainsfeature importance from the estimator and remove the least important feature\n",
    "    * repeats and removes until desired are obtained\n",
    "   RFECV can be used where no of parameters should be optimum not specified, the function performs cross validation\n",
    "2. SelectFromModel\n",
    "    * selects desired number of important features (as specified with max_features) above certain threshold of feature importance as obtained from the trained estimator\n",
    "    * feature importance is obtained via coef_, feature_importance_ or importance_getter\n",
    "    * feature importance threshold can be specified either numerically or through string argument base don built in heuristics such as 'mean', 'median' and float multiples of these like 0.1*mean\n",
    "3. Sequential Feature Selection\n",
    "    * Performs feature selection by selecting or deselecting features one by one in a greedy manner\n",
    "        * forward selection - starting with zero feature, obrains the berst class validatoin score for an estimator when trained on that featured, repeats by adding new features\n",
    "        * backward selection - starts with all and keeps removing one by one\n",
    "    * forward and backward do not yeild the same\n",
    "    * choose one which is closest from inital to end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a13815-d1dc-41a2-b1bc-78ae247d9367",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.9 - Heterogeneous features transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50f801e-157e-43cd-863c-a61176e98aaa",
   "metadata": {},
   "source": [
    "Composite transformer: \n",
    "\n",
    "sklearn.compose has useful classes and methods to apply transformation on subset of features and combine them\n",
    "\n",
    "## ColumnTransfer\n",
    "* Applies a set of transformers to columns of an array or pandas.DataFrame, concatenates the transformed outputs from different transformers into a single matrix\n",
    "* It is useful for transforming heterogeneous data by applying different transformers to separate subsets of features\n",
    "* combines different feature selection mechanisms and transformation into a single transformer object\n",
    "\n",
    "## TransformedTargetRegressor\n",
    "* Transforms target variable y before fitting a regression model\n",
    "* The predicted values are mapped back to the original space via an inverse transform\n",
    "* function takes regressor and transformer to be applied to the target variable as arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f428992f-e8d0-4bd6-ae74-0449fa61bd6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# L2.10 - Dimensionality reduction by PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2498606-5a2f-4fac-b4d8-32dafea2b366",
   "metadata": {},
   "source": [
    "unsupervised dimensionality reduction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
